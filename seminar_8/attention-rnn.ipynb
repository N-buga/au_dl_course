{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "import urllib.request\n",
    "import os\n",
    "import random\n",
    "\n",
    "class ImdbMovieReviews:\n",
    "    DEFAULT_URL = \\\n",
    "        'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    TOKEN_REGEX = re.compile(r'[A-Za-z]+|[!?.:,()]')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache_dir = './imdb'\n",
    "        self._url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "        \n",
    "        if not os.path.isfile(self._cache_dir):\n",
    "            urllib.request.urlretrieve(self._url, self._cache_dir)\n",
    "        self.filepath = self._cache_dir\n",
    "\n",
    "    def __iter__(self):\n",
    "        with tarfile.open(self.filepath) as archive:\n",
    "            items = archive.getnames()\n",
    "            for filename in archive.getnames():\n",
    "                if filename.startswith('aclImdb/train/pos/'):\n",
    "                    yield self._read(archive, filename), True\n",
    "                elif filename.startswith('aclImdb/train/neg/'):\n",
    "                    yield self._read(archive, filename), False\n",
    "                    \n",
    "    def _read(self, archive, filename):\n",
    "        with archive.extractfile(filename) as file_:\n",
    "            data = file_.read().decode('utf-8')\n",
    "            data = type(self).TOKEN_REGEX.findall(data)\n",
    "            data = [x.lower() for x in data]\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Spacy is my favourite nlp framework, which havu builtin word embeddings trains on wikipesia\n",
    "from spacy.en import English\n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self, length):\n",
    "#          spaCy makes using word vectors very easy. \n",
    "#             The Lexeme , Token , Span  and Doc  classes all have a .vector property,\n",
    "#             which is a 1-dimensional numpy array of 32-bit floats:\n",
    "        self.parser = English()\n",
    "        self._length = length\n",
    "        self.dimensions = 300\n",
    "        \n",
    "    def __call__(self, sequence):\n",
    "        data = np.zeros((self._length, self.dimensions))\n",
    "        # you can access known words from the parser's vocabulary\n",
    "        embedded = [self.parser.vocab[w].vector for w in sequence]\n",
    "        data[:len(sequence)] = embedded\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lazy import lazy\n",
    "\n",
    "class SequenceClassificationModel:\n",
    "    def __init__(self, data, params):\n",
    "        self.params = params\n",
    "        self.cpnt_path = 'att_checkpoints'\n",
    "        self.attention_size = 500\n",
    "        self._create_placeholders()\n",
    "        self.prediction\n",
    "        self.cost\n",
    "        self.error\n",
    "        self.optimize\n",
    "        self.global_step = 0\n",
    "        self._create_summaries()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _create_placeholders(self):\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.data = tf.placeholder(tf.float32, [None, self.params.seq_length, self.params.embed_length])\n",
    "            self.target = tf.placeholder(tf.float32, [None, 2])\n",
    "  \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar('loss', self.cost)\n",
    "            tf.summary.scalar('erroe', self.error)\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "    @lazy\n",
    "    def length(self):\n",
    "        with tf.name_scope(\"seq_length\"):\n",
    "            used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "            length = tf.reduce_sum(used, reduction_indices=1)\n",
    "            length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @lazy\n",
    "    def prediction(self):\n",
    "        with tf.name_scope(\"recurrent_layer\"):\n",
    "            rnn_output, _ = tf.nn.dynamic_rnn(\n",
    "                self.params.rnn_cell(self.params.rnn_hidden),\n",
    "                self.data,\n",
    "                dtype=tf.float32,\n",
    "                sequence_length=self.length\n",
    "            )\n",
    "        \n",
    "        with tf.name_scope(\"attention\"):\n",
    "            hidden_size = rnn_output.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "            # Trainable parameters\n",
    "            W_omega = tf.Variable(tf.random_normal([self.params.rnn_hidden, self.attention_size], stddev=0.1))\n",
    "            b_omega = tf.Variable(tf.random_normal([self.attention_size], stddev=0.1))\n",
    "            u_omega = tf.Variable(tf.random_normal([self.attention_size], stddev=0.1))\n",
    "\n",
    "            # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "            #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "            v = tf.tanh(tf.tensordot(rnn_output, W_omega, axes=1) + b_omega)\n",
    "            # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "            vu = tf.tensordot(v, u_omega, axes=1)   # (B,T) shape\n",
    "            alphas = tf.nn.softmax(vu)              # (B,T) shape also\n",
    "\n",
    "            # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "#             output = tf.reduce_sum(rnn_output * tf.expand_dims(alphas, -1), 1)\n",
    "            last = tf.reduce_sum(rnn_output * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "#         last = self._last_relevant(output, self.length)\n",
    "\n",
    "        with tf.name_scope(\"softmax_layer\"):\n",
    "            num_classes = int(self.target.get_shape()[1])\n",
    "            weight = tf.Variable(tf.truncated_normal(\n",
    "                [self.params.rnn_hidden, num_classes], stddev=0.01))\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "            prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "        return prediction\n",
    "    \n",
    "    @lazy\n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "    \n",
    "    @lazy\n",
    "    def error(self):\n",
    "        self.mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(self.mistakes, tf.float32))\n",
    "    \n",
    "    @lazy\n",
    "    def optimize(self):\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            gradient = self.params.optimizer.compute_gradients(self.cost)\n",
    "            if self.params.gradient_clipping:\n",
    "                limit = self.params.gradient_clipping\n",
    "                gradient = [\n",
    "                    (tf.clip_by_value(g, -limit, limit), v)\n",
    "                    if g is not None else (None, v)\n",
    "                    for g, v in gradient]\n",
    "            optimize = self.params.optimizer.apply_gradients(gradient)\n",
    "        return optimize\n",
    "    \n",
    "    def train(self, batches, save_prefix, save_every=10):\n",
    "        saver = tf.train.Saver()\n",
    "        if os.path.isdir(self.cpnt_path):\n",
    "            saver.restore(self.sess, tf.train.latest_checkpoint(self.cpnt_path))\n",
    "        else:\n",
    "            os.makedirs(self.cpnt_path)\n",
    "        summary_path = os.path.join('att_graphs', 'run{}'.format(self.global_step))\n",
    "        summary_writer = tf.summary.FileWriter(summary_path, self.sess.graph)\n",
    "        self.global_step += 1\n",
    "        for index, batch in enumerate(batches):\n",
    "            feed = {model.data: batch[0], model.target: batch[1]}\n",
    "            error, _, summary_str = self.sess.run([model.error, model.optimize, model.summary], feed)\n",
    "            print('{}: {:3.1f}%'.format(index + 1, 100 * error))\n",
    "            if index % save_every == 0:\n",
    "                summary_writer.add_summary(summary_str, index)\n",
    "                summary_writer.flush()\n",
    "            if index % save_every == 0:\n",
    "                save_path = os.path.join(self.cpnt_path, save_prefix)\n",
    "                print('saving...', save_path)\n",
    "                saver.save(self.sess, save_path, global_step=index)\n",
    "                \n",
    "        saver.save(self.sess, os.path.join(self.cpnt_path, save_prefix + '_final'))\n",
    "\n",
    "    def predict_proba(self, data):\n",
    "        feed = {model.data: data, }\n",
    "        prediction = self.sess.run([model.prediction], feed)        \n",
    "        return prediction\n",
    "        \n",
    "    def close(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_batched(iterator, length, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        data = np.zeros((batch_size, length, embedding.dimensions))\n",
    "        target = np.zeros((batch_size, 2))\n",
    "        for index in range(batch_size):\n",
    "            text, label = next(iterator)\n",
    "            data[index] = embedding(text)\n",
    "            target[index] = [1, 0] if label else [0, 1]\n",
    "        yield data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = list(ImdbMovieReviews())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = max(len(x[0]) for x in reviews)\n",
    "embedding = Embedding(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from attrdict import AttrDict\n",
    "\n",
    "params = AttrDict(\n",
    "    rnn_cell=tf.contrib.rnn.GRUCell,\n",
    "    rnn_hidden=300,\n",
    "    optimizer=tf.train.RMSPropOptimizer(0.002),\n",
    "    batch_size=20,\n",
    "    gradient_clipping=100,\n",
    "    seq_length=length,\n",
    "    embed_length=embedding.dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = preprocess_batched(reviews, length, embedding, params.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = SequenceClassificationModel(data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 50.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "2: 60.0%\n",
      "3: 50.0%\n",
      "4: 50.0%\n",
      "5: 50.0%\n",
      "6: 50.0%\n",
      "7: 60.0%\n",
      "8: 55.0%\n",
      "9: 55.0%\n",
      "10: 35.0%\n",
      "11: 65.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "12: 55.0%\n",
      "13: 35.0%\n",
      "14: 25.0%\n",
      "15: 55.0%\n",
      "16: 45.0%\n",
      "17: 55.0%\n",
      "18: 35.0%\n",
      "19: 60.0%\n",
      "20: 40.0%\n",
      "21: 40.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "22: 60.0%\n",
      "23: 50.0%\n",
      "24: 40.0%\n",
      "25: 70.0%\n",
      "26: 40.0%\n",
      "27: 45.0%\n",
      "28: 55.0%\n",
      "29: 55.0%\n",
      "30: 50.0%\n",
      "31: 50.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "32: 45.0%\n",
      "33: 50.0%\n",
      "34: 40.0%\n",
      "35: 55.0%\n",
      "36: 45.0%\n",
      "37: 45.0%\n",
      "38: 60.0%\n",
      "39: 35.0%\n",
      "40: 55.0%\n",
      "41: 50.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "42: 30.0%\n",
      "43: 50.0%\n",
      "44: 50.0%\n",
      "45: 50.0%\n",
      "46: 35.0%\n",
      "47: 60.0%\n",
      "48: 55.0%\n",
      "49: 55.0%\n",
      "50: 65.0%\n",
      "51: 35.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "52: 50.0%\n",
      "53: 40.0%\n",
      "54: 50.0%\n",
      "55: 60.0%\n",
      "56: 60.0%\n",
      "57: 50.0%\n",
      "58: 55.0%\n",
      "59: 50.0%\n",
      "60: 65.0%\n",
      "61: 65.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "62: 65.0%\n",
      "63: 60.0%\n",
      "64: 55.0%\n",
      "65: 55.0%\n",
      "66: 40.0%\n",
      "67: 60.0%\n",
      "68: 60.0%\n",
      "69: 60.0%\n",
      "70: 45.0%\n",
      "71: 65.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "72: 65.0%\n",
      "73: 40.0%\n",
      "74: 55.0%\n",
      "75: 70.0%\n",
      "76: 55.0%\n",
      "77: 55.0%\n",
      "78: 60.0%\n",
      "79: 55.0%\n",
      "80: 65.0%\n",
      "81: 40.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "82: 40.0%\n",
      "83: 25.0%\n",
      "84: 40.0%\n",
      "85: 65.0%\n",
      "86: 50.0%\n",
      "87: 50.0%\n",
      "88: 45.0%\n",
      "89: 40.0%\n",
      "90: 45.0%\n",
      "91: 65.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "92: 40.0%\n",
      "93: 60.0%\n",
      "94: 65.0%\n",
      "95: 30.0%\n",
      "96: 50.0%\n",
      "97: 55.0%\n",
      "98: 60.0%\n",
      "99: 60.0%\n",
      "100: 55.0%\n",
      "101: 55.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "102: 60.0%\n",
      "103: 40.0%\n",
      "104: 35.0%\n",
      "105: 65.0%\n",
      "106: 45.0%\n",
      "107: 45.0%\n",
      "108: 65.0%\n",
      "109: 65.0%\n",
      "110: 45.0%\n",
      "111: 55.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "112: 50.0%\n",
      "113: 50.0%\n",
      "114: 35.0%\n",
      "115: 75.0%\n",
      "116: 55.0%\n",
      "117: 50.0%\n",
      "118: 40.0%\n",
      "119: 30.0%\n",
      "120: 50.0%\n",
      "121: 45.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "122: 50.0%\n",
      "123: 45.0%\n",
      "124: 40.0%\n",
      "125: 30.0%\n",
      "126: 50.0%\n",
      "127: 45.0%\n",
      "128: 55.0%\n",
      "129: 60.0%\n",
      "130: 50.0%\n",
      "131: 60.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "132: 55.0%\n",
      "133: 50.0%\n",
      "134: 60.0%\n",
      "135: 40.0%\n",
      "136: 55.0%\n",
      "137: 50.0%\n",
      "138: 55.0%\n",
      "139: 45.0%\n",
      "140: 65.0%\n",
      "141: 50.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "142: 45.0%\n",
      "143: 40.0%\n",
      "144: 70.0%\n",
      "145: 40.0%\n",
      "146: 40.0%\n",
      "147: 40.0%\n",
      "148: 75.0%\n",
      "149: 60.0%\n",
      "150: 60.0%\n",
      "151: 75.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "152: 35.0%\n",
      "153: 50.0%\n",
      "154: 50.0%\n",
      "155: 55.0%\n",
      "156: 45.0%\n",
      "157: 55.0%\n",
      "158: 60.0%\n",
      "159: 45.0%\n",
      "160: 65.0%\n",
      "161: 50.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "162: 70.0%\n",
      "163: 50.0%\n",
      "164: 50.0%\n",
      "165: 45.0%\n",
      "166: 30.0%\n",
      "167: 65.0%\n",
      "168: 40.0%\n",
      "169: 50.0%\n",
      "170: 45.0%\n",
      "171: 50.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "172: 45.0%\n",
      "173: 65.0%\n",
      "174: 50.0%\n",
      "175: 45.0%\n",
      "176: 35.0%\n",
      "177: 40.0%\n",
      "178: 55.0%\n",
      "179: 60.0%\n",
      "180: 60.0%\n",
      "181: 50.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "182: 40.0%\n",
      "183: 40.0%\n",
      "184: 55.0%\n",
      "185: 55.0%\n",
      "186: 45.0%\n",
      "187: 40.0%\n",
      "188: 65.0%\n",
      "189: 50.0%\n",
      "190: 55.0%\n",
      "191: 45.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "192: 60.0%\n",
      "193: 65.0%\n",
      "194: 55.0%\n",
      "195: 30.0%\n",
      "196: 60.0%\n",
      "197: 70.0%\n",
      "198: 50.0%\n",
      "199: 50.0%\n",
      "200: 45.0%\n",
      "201: 50.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "202: 45.0%\n",
      "203: 45.0%\n",
      "204: 70.0%\n",
      "205: 65.0%\n",
      "206: 45.0%\n",
      "207: 60.0%\n",
      "208: 45.0%\n",
      "209: 45.0%\n",
      "210: 45.0%\n",
      "211: 60.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "212: 55.0%\n",
      "213: 50.0%\n",
      "214: 50.0%\n",
      "215: 60.0%\n",
      "216: 25.0%\n",
      "217: 55.0%\n",
      "218: 70.0%\n",
      "219: 50.0%\n",
      "220: 40.0%\n",
      "221: 40.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "222: 65.0%\n",
      "223: 45.0%\n",
      "224: 45.0%\n",
      "225: 50.0%\n",
      "226: 25.0%\n",
      "227: 60.0%\n",
      "228: 35.0%\n",
      "229: 55.0%\n",
      "230: 50.0%\n",
      "231: 65.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "232: 35.0%\n",
      "233: 45.0%\n",
      "234: 35.0%\n",
      "235: 45.0%\n",
      "236: 50.0%\n",
      "237: 55.0%\n",
      "238: 30.0%\n",
      "239: 55.0%\n",
      "240: 50.0%\n",
      "241: 35.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "242: 55.0%\n",
      "243: 75.0%\n",
      "244: 75.0%\n",
      "245: 35.0%\n",
      "246: 40.0%\n",
      "247: 45.0%\n",
      "248: 45.0%\n",
      "249: 45.0%\n",
      "250: 50.0%\n",
      "251: 25.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "252: 40.0%\n",
      "253: 50.0%\n",
      "254: 45.0%\n",
      "255: 70.0%\n",
      "256: 45.0%\n",
      "257: 75.0%\n",
      "258: 35.0%\n",
      "259: 50.0%\n",
      "260: 55.0%\n",
      "261: 60.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "262: 40.0%\n",
      "263: 55.0%\n",
      "264: 50.0%\n",
      "265: 35.0%\n",
      "266: 45.0%\n",
      "267: 70.0%\n",
      "268: 30.0%\n",
      "269: 55.0%\n",
      "270: 45.0%\n",
      "271: 45.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "272: 40.0%\n",
      "273: 55.0%\n",
      "274: 50.0%\n",
      "275: 60.0%\n",
      "276: 30.0%\n",
      "277: 60.0%\n",
      "278: 40.0%\n",
      "279: 70.0%\n",
      "280: 40.0%\n",
      "281: 30.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "282: 70.0%\n",
      "283: 55.0%\n",
      "284: 60.0%\n",
      "285: 50.0%\n",
      "286: 65.0%\n",
      "287: 55.0%\n",
      "288: 60.0%\n",
      "289: 40.0%\n",
      "290: 55.0%\n",
      "291: 40.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "292: 65.0%\n",
      "293: 55.0%\n",
      "294: 50.0%\n",
      "295: 55.0%\n",
      "296: 40.0%\n",
      "297: 50.0%\n",
      "298: 55.0%\n",
      "299: 45.0%\n",
      "300: 40.0%\n",
      "301: 45.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "302: 45.0%\n",
      "303: 65.0%\n",
      "304: 40.0%\n",
      "305: 65.0%\n",
      "306: 50.0%\n",
      "307: 35.0%\n",
      "308: 50.0%\n",
      "309: 45.0%\n",
      "310: 50.0%\n",
      "311: 60.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "312: 25.0%\n",
      "313: 50.0%\n",
      "314: 50.0%\n",
      "315: 50.0%\n",
      "316: 55.0%\n",
      "317: 35.0%\n",
      "318: 55.0%\n",
      "319: 75.0%\n",
      "320: 35.0%\n",
      "321: 35.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "322: 65.0%\n",
      "323: 50.0%\n",
      "324: 60.0%\n",
      "325: 50.0%\n",
      "326: 75.0%\n",
      "327: 50.0%\n",
      "328: 45.0%\n",
      "329: 55.0%\n",
      "330: 60.0%\n",
      "331: 60.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "332: 25.0%\n",
      "333: 50.0%\n",
      "334: 55.0%\n",
      "335: 45.0%\n",
      "336: 50.0%\n",
      "337: 60.0%\n",
      "338: 50.0%\n",
      "339: 60.0%\n",
      "340: 60.0%\n",
      "341: 60.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "342: 55.0%\n",
      "343: 60.0%\n",
      "344: 40.0%\n",
      "345: 60.0%\n",
      "346: 35.0%\n",
      "347: 30.0%\n",
      "348: 50.0%\n",
      "349: 40.0%\n",
      "350: 45.0%\n",
      "351: 60.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "352: 50.0%\n",
      "353: 30.0%\n",
      "354: 60.0%\n",
      "355: 35.0%\n",
      "356: 40.0%\n",
      "357: 50.0%\n",
      "358: 35.0%\n",
      "359: 55.0%\n",
      "360: 45.0%\n",
      "361: 55.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "362: 35.0%\n",
      "363: 55.0%\n",
      "364: 40.0%\n",
      "365: 45.0%\n",
      "366: 50.0%\n",
      "367: 40.0%\n",
      "368: 45.0%\n",
      "369: 50.0%\n",
      "370: 50.0%\n",
      "371: 65.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "372: 40.0%\n",
      "373: 65.0%\n",
      "374: 65.0%\n",
      "375: 30.0%\n",
      "376: 60.0%\n",
      "377: 40.0%\n",
      "378: 45.0%\n",
      "379: 20.0%\n",
      "380: 70.0%\n",
      "381: 65.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "382: 65.0%\n",
      "383: 40.0%\n",
      "384: 80.0%\n",
      "385: 45.0%\n",
      "386: 60.0%\n",
      "387: 50.0%\n",
      "388: 65.0%\n",
      "389: 45.0%\n",
      "390: 40.0%\n",
      "391: 80.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "392: 50.0%\n",
      "393: 55.0%\n",
      "394: 55.0%\n",
      "395: 55.0%\n",
      "396: 60.0%\n",
      "397: 35.0%\n",
      "398: 65.0%\n",
      "399: 40.0%\n",
      "400: 40.0%\n",
      "401: 65.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "402: 40.0%\n",
      "403: 60.0%\n",
      "404: 45.0%\n",
      "405: 50.0%\n",
      "406: 50.0%\n",
      "407: 40.0%\n",
      "408: 40.0%\n",
      "409: 55.0%\n",
      "410: 20.0%\n",
      "411: 70.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "412: 50.0%\n",
      "413: 50.0%\n",
      "414: 45.0%\n",
      "415: 35.0%\n",
      "416: 75.0%\n",
      "417: 65.0%\n",
      "418: 50.0%\n",
      "419: 70.0%\n",
      "420: 75.0%\n",
      "421: 50.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "422: 50.0%\n",
      "423: 40.0%\n",
      "424: 60.0%\n",
      "425: 45.0%\n",
      "426: 40.0%\n",
      "427: 40.0%\n",
      "428: 60.0%\n",
      "429: 40.0%\n",
      "430: 40.0%\n",
      "431: 40.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "432: 45.0%\n",
      "433: 55.0%\n",
      "434: 60.0%\n",
      "435: 80.0%\n",
      "436: 30.0%\n",
      "437: 40.0%\n",
      "438: 50.0%\n",
      "439: 30.0%\n",
      "440: 45.0%\n",
      "441: 65.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "442: 50.0%\n",
      "443: 45.0%\n",
      "444: 40.0%\n",
      "445: 50.0%\n",
      "446: 35.0%\n",
      "447: 45.0%\n",
      "448: 45.0%\n",
      "449: 40.0%\n",
      "450: 45.0%\n",
      "451: 50.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "452: 55.0%\n",
      "453: 55.0%\n",
      "454: 55.0%\n",
      "455: 60.0%\n",
      "456: 40.0%\n",
      "457: 55.0%\n",
      "458: 40.0%\n",
      "459: 35.0%\n",
      "460: 45.0%\n",
      "461: 35.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "462: 25.0%\n",
      "463: 45.0%\n",
      "464: 50.0%\n",
      "465: 50.0%\n",
      "466: 60.0%\n",
      "467: 45.0%\n",
      "468: 40.0%\n",
      "469: 30.0%\n",
      "470: 45.0%\n",
      "471: 45.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "472: 35.0%\n",
      "473: 40.0%\n",
      "474: 65.0%\n",
      "475: 35.0%\n",
      "476: 40.0%\n",
      "477: 50.0%\n",
      "478: 40.0%\n",
      "479: 35.0%\n",
      "480: 50.0%\n",
      "481: 50.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "482: 50.0%\n",
      "483: 35.0%\n",
      "484: 35.0%\n",
      "485: 45.0%\n",
      "486: 40.0%\n",
      "487: 45.0%\n",
      "488: 55.0%\n",
      "489: 50.0%\n",
      "490: 60.0%\n",
      "491: 45.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "492: 25.0%\n",
      "493: 55.0%\n",
      "494: 35.0%\n",
      "495: 65.0%\n",
      "496: 35.0%\n",
      "497: 55.0%\n",
      "498: 45.0%\n",
      "499: 40.0%\n",
      "500: 55.0%\n",
      "501: 40.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "502: 40.0%\n",
      "503: 45.0%\n",
      "504: 40.0%\n",
      "505: 55.0%\n",
      "506: 65.0%\n",
      "507: 55.0%\n",
      "508: 40.0%\n",
      "509: 35.0%\n",
      "510: 50.0%\n",
      "511: 70.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "512: 40.0%\n",
      "513: 45.0%\n",
      "514: 60.0%\n",
      "515: 50.0%\n",
      "516: 30.0%\n",
      "517: 30.0%\n",
      "518: 60.0%\n",
      "519: 40.0%\n",
      "520: 35.0%\n",
      "521: 25.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "522: 55.0%\n",
      "523: 60.0%\n",
      "524: 60.0%\n",
      "525: 55.0%\n",
      "526: 75.0%\n",
      "527: 35.0%\n",
      "528: 40.0%\n",
      "529: 25.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530: 60.0%\n",
      "531: 40.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "532: 30.0%\n",
      "533: 35.0%\n",
      "534: 55.0%\n",
      "535: 25.0%\n",
      "536: 50.0%\n",
      "537: 50.0%\n",
      "538: 25.0%\n",
      "539: 45.0%\n",
      "540: 45.0%\n",
      "541: 25.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "542: 15.0%\n",
      "543: 55.0%\n",
      "544: 50.0%\n",
      "545: 55.0%\n",
      "546: 20.0%\n",
      "547: 35.0%\n",
      "548: 35.0%\n",
      "549: 45.0%\n",
      "550: 50.0%\n",
      "551: 25.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "552: 40.0%\n",
      "553: 30.0%\n",
      "554: 10.0%\n",
      "555: 25.0%\n",
      "556: 40.0%\n",
      "557: 35.0%\n",
      "558: 20.0%\n",
      "559: 30.0%\n",
      "560: 55.0%\n",
      "561: 25.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "562: 35.0%\n",
      "563: 40.0%\n",
      "564: 20.0%\n",
      "565: 35.0%\n",
      "566: 35.0%\n",
      "567: 55.0%\n",
      "568: 50.0%\n",
      "569: 45.0%\n",
      "570: 40.0%\n",
      "571: 40.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "572: 50.0%\n",
      "573: 25.0%\n",
      "574: 25.0%\n",
      "575: 15.0%\n",
      "576: 45.0%\n",
      "577: 50.0%\n",
      "578: 20.0%\n",
      "579: 35.0%\n",
      "580: 40.0%\n",
      "581: 30.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "582: 45.0%\n",
      "583: 55.0%\n",
      "584: 15.0%\n",
      "585: 35.0%\n",
      "586: 30.0%\n",
      "587: 30.0%\n",
      "588: 45.0%\n",
      "589: 60.0%\n",
      "590: 35.0%\n",
      "591: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "592: 30.0%\n",
      "593: 45.0%\n",
      "594: 50.0%\n",
      "595: 35.0%\n",
      "596: 35.0%\n",
      "597: 40.0%\n",
      "598: 40.0%\n",
      "599: 40.0%\n",
      "600: 25.0%\n",
      "601: 25.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "602: 20.0%\n",
      "603: 20.0%\n",
      "604: 20.0%\n",
      "605: 45.0%\n",
      "606: 30.0%\n",
      "607: 50.0%\n",
      "608: 35.0%\n",
      "609: 30.0%\n",
      "610: 35.0%\n",
      "611: 30.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "612: 10.0%\n",
      "613: 25.0%\n",
      "614: 35.0%\n",
      "615: 45.0%\n",
      "616: 30.0%\n",
      "617: 25.0%\n",
      "618: 35.0%\n",
      "619: 20.0%\n",
      "620: 15.0%\n",
      "621: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "622: 35.0%\n",
      "623: 45.0%\n",
      "624: 35.0%\n",
      "625: 45.0%\n",
      "626: 30.0%\n",
      "627: 25.0%\n",
      "628: 20.0%\n",
      "629: 25.0%\n",
      "630: 35.0%\n",
      "631: 30.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "632: 40.0%\n",
      "633: 10.0%\n",
      "634: 45.0%\n",
      "635: 20.0%\n",
      "636: 15.0%\n",
      "637: 40.0%\n",
      "638: 30.0%\n",
      "639: 25.0%\n",
      "640: 20.0%\n",
      "641: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "642: 20.0%\n",
      "643: 10.0%\n",
      "644: 25.0%\n",
      "645: 30.0%\n",
      "646: 20.0%\n",
      "647: 40.0%\n",
      "648: 15.0%\n",
      "649: 15.0%\n",
      "650: 20.0%\n",
      "651: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "652: 35.0%\n",
      "653: 30.0%\n",
      "654: 50.0%\n",
      "655: 30.0%\n",
      "656: 30.0%\n",
      "657: 40.0%\n",
      "658: 20.0%\n",
      "659: 20.0%\n",
      "660: 15.0%\n",
      "661: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "662: 15.0%\n",
      "663: 20.0%\n",
      "664: 35.0%\n",
      "665: 25.0%\n",
      "666: 15.0%\n",
      "667: 25.0%\n",
      "668: 30.0%\n",
      "669: 45.0%\n",
      "670: 35.0%\n",
      "671: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "672: 35.0%\n",
      "673: 15.0%\n",
      "674: 30.0%\n",
      "675: 20.0%\n",
      "676: 15.0%\n",
      "677: 20.0%\n",
      "678: 20.0%\n",
      "679: 30.0%\n",
      "680: 30.0%\n",
      "681: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "682: 25.0%\n",
      "683: 15.0%\n",
      "684: 20.0%\n",
      "685: 15.0%\n",
      "686: 20.0%\n",
      "687: 20.0%\n",
      "688: 15.0%\n",
      "689: 20.0%\n",
      "690: 25.0%\n",
      "691: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "692: 15.0%\n",
      "693: 30.0%\n",
      "694: 20.0%\n",
      "695: 25.0%\n",
      "696: 25.0%\n",
      "697: 30.0%\n",
      "698: 20.0%\n",
      "699: 15.0%\n",
      "700: 25.0%\n",
      "701: 10.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "702: 10.0%\n",
      "703: 30.0%\n",
      "704: 15.0%\n",
      "705: 25.0%\n",
      "706: 25.0%\n",
      "707: 20.0%\n",
      "708: 10.0%\n",
      "709: 25.0%\n",
      "710: 15.0%\n",
      "711: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "712: 15.0%\n",
      "713: 30.0%\n",
      "714: 20.0%\n",
      "715: 15.0%\n",
      "716: 30.0%\n",
      "717: 15.0%\n",
      "718: 15.0%\n",
      "719: 15.0%\n",
      "720: 25.0%\n",
      "721: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "722: 10.0%\n",
      "723: 15.0%\n",
      "724: 10.0%\n",
      "725: 45.0%\n",
      "726: 10.0%\n",
      "727: 35.0%\n",
      "728: 30.0%\n",
      "729: 20.0%\n",
      "730: 15.0%\n",
      "731: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "732: 30.0%\n",
      "733: 20.0%\n",
      "734: 10.0%\n",
      "735: 20.0%\n",
      "736: 15.0%\n",
      "737: 20.0%\n",
      "738: 20.0%\n",
      "739: 20.0%\n",
      "740: 35.0%\n",
      "741: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "742: 20.0%\n",
      "743: 25.0%\n",
      "744: 10.0%\n",
      "745: 25.0%\n",
      "746: 20.0%\n",
      "747: 15.0%\n",
      "748: 35.0%\n",
      "749: 25.0%\n",
      "750: 20.0%\n",
      "751: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "752: 25.0%\n",
      "753: 5.0%\n",
      "754: 35.0%\n",
      "755: 20.0%\n",
      "756: 20.0%\n",
      "757: 15.0%\n",
      "758: 20.0%\n",
      "759: 20.0%\n",
      "760: 10.0%\n",
      "761: 10.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "762: 0.0%\n",
      "763: 20.0%\n",
      "764: 15.0%\n",
      "765: 25.0%\n",
      "766: 5.0%\n",
      "767: 10.0%\n",
      "768: 10.0%\n",
      "769: 15.0%\n",
      "770: 20.0%\n",
      "771: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "772: 30.0%\n",
      "773: 15.0%\n",
      "774: 25.0%\n",
      "775: 15.0%\n",
      "776: 10.0%\n",
      "777: 30.0%\n",
      "778: 5.0%\n",
      "779: 25.0%\n",
      "780: 15.0%\n",
      "781: 35.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "782: 5.0%\n",
      "783: 10.0%\n",
      "784: 10.0%\n",
      "785: 5.0%\n",
      "786: 30.0%\n",
      "787: 35.0%\n",
      "788: 30.0%\n",
      "789: 20.0%\n",
      "790: 25.0%\n",
      "791: 10.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "792: 20.0%\n",
      "793: 30.0%\n",
      "794: 20.0%\n",
      "795: 25.0%\n",
      "796: 30.0%\n",
      "797: 10.0%\n",
      "798: 10.0%\n",
      "799: 20.0%\n",
      "800: 15.0%\n",
      "801: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "802: 25.0%\n",
      "803: 15.0%\n",
      "804: 15.0%\n",
      "805: 25.0%\n",
      "806: 30.0%\n",
      "807: 20.0%\n",
      "808: 30.0%\n",
      "809: 20.0%\n",
      "810: 15.0%\n",
      "811: 40.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "812: 20.0%\n",
      "813: 5.0%\n",
      "814: 10.0%\n",
      "815: 10.0%\n",
      "816: 15.0%\n",
      "817: 15.0%\n",
      "818: 15.0%\n",
      "819: 15.0%\n",
      "820: 40.0%\n",
      "821: 35.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "822: 5.0%\n",
      "823: 15.0%\n",
      "824: 15.0%\n",
      "825: 20.0%\n",
      "826: 15.0%\n",
      "827: 5.0%\n",
      "828: 10.0%\n",
      "829: 10.0%\n",
      "830: 15.0%\n",
      "831: 10.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "832: 5.0%\n",
      "833: 15.0%\n",
      "834: 25.0%\n",
      "835: 20.0%\n",
      "836: 20.0%\n",
      "837: 25.0%\n",
      "838: 30.0%\n",
      "839: 30.0%\n",
      "840: 0.0%\n",
      "841: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "842: 20.0%\n",
      "843: 15.0%\n",
      "844: 10.0%\n",
      "845: 10.0%\n",
      "846: 25.0%\n",
      "847: 25.0%\n",
      "848: 5.0%\n",
      "849: 10.0%\n",
      "850: 25.0%\n",
      "851: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "852: 30.0%\n",
      "853: 20.0%\n",
      "854: 10.0%\n",
      "855: 10.0%\n",
      "856: 20.0%\n",
      "857: 20.0%\n",
      "858: 15.0%\n",
      "859: 20.0%\n",
      "860: 15.0%\n",
      "861: 35.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "862: 15.0%\n",
      "863: 10.0%\n",
      "864: 20.0%\n",
      "865: 15.0%\n",
      "866: 30.0%\n",
      "867: 20.0%\n",
      "868: 10.0%\n",
      "869: 35.0%\n",
      "870: 5.0%\n",
      "871: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "872: 20.0%\n",
      "873: 15.0%\n",
      "874: 25.0%\n",
      "875: 15.0%\n",
      "876: 5.0%\n",
      "877: 55.0%\n",
      "878: 15.0%\n",
      "879: 5.0%\n",
      "880: 20.0%\n",
      "881: 0.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "882: 15.0%\n",
      "883: 10.0%\n",
      "884: 15.0%\n",
      "885: 10.0%\n",
      "886: 20.0%\n",
      "887: 20.0%\n",
      "888: 30.0%\n",
      "889: 10.0%\n",
      "890: 20.0%\n",
      "891: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "892: 10.0%\n",
      "893: 15.0%\n",
      "894: 20.0%\n",
      "895: 30.0%\n",
      "896: 30.0%\n",
      "897: 15.0%\n",
      "898: 20.0%\n",
      "899: 5.0%\n",
      "900: 5.0%\n",
      "901: 10.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "902: 10.0%\n",
      "903: 5.0%\n",
      "904: 20.0%\n",
      "905: 15.0%\n",
      "906: 20.0%\n",
      "907: 25.0%\n",
      "908: 10.0%\n",
      "909: 20.0%\n",
      "910: 0.0%\n",
      "911: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "912: 15.0%\n",
      "913: 15.0%\n",
      "914: 20.0%\n",
      "915: 10.0%\n",
      "916: 25.0%\n",
      "917: 10.0%\n",
      "918: 25.0%\n",
      "919: 0.0%\n",
      "920: 30.0%\n",
      "921: 5.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "922: 10.0%\n",
      "923: 5.0%\n",
      "924: 25.0%\n",
      "925: 5.0%\n",
      "926: 25.0%\n",
      "927: 5.0%\n",
      "928: 10.0%\n",
      "929: 10.0%\n",
      "930: 10.0%\n",
      "931: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "932: 10.0%\n",
      "933: 25.0%\n",
      "934: 35.0%\n",
      "935: 10.0%\n",
      "936: 15.0%\n",
      "937: 10.0%\n",
      "938: 5.0%\n",
      "939: 20.0%\n",
      "940: 15.0%\n",
      "941: 5.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "942: 5.0%\n",
      "943: 15.0%\n",
      "944: 10.0%\n",
      "945: 10.0%\n",
      "946: 25.0%\n",
      "947: 35.0%\n",
      "948: 10.0%\n",
      "949: 10.0%\n",
      "950: 35.0%\n",
      "951: 10.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "952: 20.0%\n",
      "953: 20.0%\n",
      "954: 15.0%\n",
      "955: 20.0%\n",
      "956: 15.0%\n",
      "957: 25.0%\n",
      "958: 5.0%\n",
      "959: 20.0%\n",
      "960: 20.0%\n",
      "961: 30.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "962: 30.0%\n",
      "963: 5.0%\n",
      "964: 10.0%\n",
      "965: 35.0%\n",
      "966: 0.0%\n",
      "967: 20.0%\n",
      "968: 20.0%\n",
      "969: 45.0%\n",
      "970: 15.0%\n",
      "971: 0.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "972: 30.0%\n",
      "973: 30.0%\n",
      "974: 20.0%\n",
      "975: 25.0%\n",
      "976: 5.0%\n",
      "977: 15.0%\n",
      "978: 25.0%\n",
      "979: 15.0%\n",
      "980: 25.0%\n",
      "981: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "982: 20.0%\n",
      "983: 5.0%\n",
      "984: 10.0%\n",
      "985: 15.0%\n",
      "986: 20.0%\n",
      "987: 5.0%\n",
      "988: 25.0%\n",
      "989: 25.0%\n",
      "990: 10.0%\n",
      "991: 10.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "992: 20.0%\n",
      "993: 25.0%\n",
      "994: 25.0%\n",
      "995: 10.0%\n",
      "996: 0.0%\n",
      "997: 0.0%\n",
      "998: 20.0%\n",
      "999: 20.0%\n",
      "1000: 20.0%\n",
      "1001: 35.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1002: 20.0%\n",
      "1003: 20.0%\n",
      "1004: 0.0%\n",
      "1005: 5.0%\n",
      "1006: 10.0%\n",
      "1007: 30.0%\n",
      "1008: 20.0%\n",
      "1009: 20.0%\n",
      "1010: 15.0%\n",
      "1011: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1012: 20.0%\n",
      "1013: 15.0%\n",
      "1014: 20.0%\n",
      "1015: 15.0%\n",
      "1016: 15.0%\n",
      "1017: 10.0%\n",
      "1018: 15.0%\n",
      "1019: 15.0%\n",
      "1020: 15.0%\n",
      "1021: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1022: 5.0%\n",
      "1023: 15.0%\n",
      "1024: 30.0%\n",
      "1025: 5.0%\n",
      "1026: 10.0%\n",
      "1027: 20.0%\n",
      "1028: 15.0%\n",
      "1029: 5.0%\n",
      "1030: 20.0%\n",
      "1031: 30.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1032: 20.0%\n",
      "1033: 10.0%\n",
      "1034: 0.0%\n",
      "1035: 15.0%\n",
      "1036: 25.0%\n",
      "1037: 5.0%\n",
      "1038: 25.0%\n",
      "1039: 20.0%\n",
      "1040: 20.0%\n",
      "1041: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1042: 5.0%\n",
      "1043: 25.0%\n",
      "1044: 0.0%\n",
      "1045: 20.0%\n",
      "1046: 35.0%\n",
      "1047: 15.0%\n",
      "1048: 10.0%\n",
      "1049: 25.0%\n",
      "1050: 10.0%\n",
      "1051: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052: 20.0%\n",
      "1053: 15.0%\n",
      "1054: 5.0%\n",
      "1055: 5.0%\n",
      "1056: 25.0%\n",
      "1057: 10.0%\n",
      "1058: 15.0%\n",
      "1059: 15.0%\n",
      "1060: 30.0%\n",
      "1061: 10.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1062: 15.0%\n",
      "1063: 30.0%\n",
      "1064: 35.0%\n",
      "1065: 20.0%\n",
      "1066: 20.0%\n",
      "1067: 5.0%\n",
      "1068: 10.0%\n",
      "1069: 10.0%\n",
      "1070: 15.0%\n",
      "1071: 5.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1072: 20.0%\n",
      "1073: 15.0%\n",
      "1074: 20.0%\n",
      "1075: 20.0%\n",
      "1076: 30.0%\n",
      "1077: 20.0%\n",
      "1078: 30.0%\n",
      "1079: 30.0%\n",
      "1080: 15.0%\n",
      "1081: 5.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1082: 15.0%\n",
      "1083: 20.0%\n",
      "1084: 15.0%\n",
      "1085: 15.0%\n",
      "1086: 15.0%\n",
      "1087: 0.0%\n",
      "1088: 10.0%\n",
      "1089: 5.0%\n",
      "1090: 25.0%\n",
      "1091: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1092: 15.0%\n",
      "1093: 15.0%\n",
      "1094: 5.0%\n",
      "1095: 5.0%\n",
      "1096: 10.0%\n",
      "1097: 25.0%\n",
      "1098: 25.0%\n",
      "1099: 10.0%\n",
      "1100: 20.0%\n",
      "1101: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1102: 20.0%\n",
      "1103: 35.0%\n",
      "1104: 10.0%\n",
      "1105: 10.0%\n",
      "1106: 15.0%\n",
      "1107: 10.0%\n",
      "1108: 15.0%\n",
      "1109: 25.0%\n",
      "1110: 10.0%\n",
      "1111: 10.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1112: 10.0%\n",
      "1113: 15.0%\n",
      "1114: 5.0%\n",
      "1115: 20.0%\n",
      "1116: 30.0%\n",
      "1117: 5.0%\n",
      "1118: 15.0%\n",
      "1119: 10.0%\n",
      "1120: 25.0%\n",
      "1121: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1122: 30.0%\n",
      "1123: 15.0%\n",
      "1124: 25.0%\n",
      "1125: 10.0%\n",
      "1126: 15.0%\n",
      "1127: 5.0%\n",
      "1128: 0.0%\n",
      "1129: 30.0%\n",
      "1130: 10.0%\n",
      "1131: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1132: 35.0%\n",
      "1133: 35.0%\n",
      "1134: 5.0%\n",
      "1135: 20.0%\n",
      "1136: 5.0%\n",
      "1137: 35.0%\n",
      "1138: 5.0%\n",
      "1139: 15.0%\n",
      "1140: 15.0%\n",
      "1141: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1142: 35.0%\n",
      "1143: 5.0%\n",
      "1144: 15.0%\n",
      "1145: 25.0%\n",
      "1146: 15.0%\n",
      "1147: 25.0%\n",
      "1148: 10.0%\n",
      "1149: 15.0%\n",
      "1150: 20.0%\n",
      "1151: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1152: 15.0%\n",
      "1153: 10.0%\n",
      "1154: 15.0%\n",
      "1155: 15.0%\n",
      "1156: 5.0%\n",
      "1157: 20.0%\n",
      "1158: 5.0%\n",
      "1159: 5.0%\n",
      "1160: 15.0%\n",
      "1161: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1162: 5.0%\n",
      "1163: 15.0%\n",
      "1164: 10.0%\n",
      "1165: 20.0%\n",
      "1166: 25.0%\n",
      "1167: 10.0%\n",
      "1168: 20.0%\n",
      "1169: 5.0%\n",
      "1170: 10.0%\n",
      "1171: 0.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1172: 20.0%\n",
      "1173: 30.0%\n",
      "1174: 25.0%\n",
      "1175: 5.0%\n",
      "1176: 10.0%\n",
      "1177: 15.0%\n",
      "1178: 15.0%\n",
      "1179: 0.0%\n",
      "1180: 15.0%\n",
      "1181: 35.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1182: 10.0%\n",
      "1183: 15.0%\n",
      "1184: 5.0%\n",
      "1185: 10.0%\n",
      "1186: 10.0%\n",
      "1187: 15.0%\n",
      "1188: 40.0%\n",
      "1189: 15.0%\n",
      "1190: 5.0%\n",
      "1191: 15.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1192: 15.0%\n",
      "1193: 20.0%\n",
      "1194: 15.0%\n",
      "1195: 20.0%\n",
      "1196: 5.0%\n",
      "1197: 15.0%\n",
      "1198: 5.0%\n",
      "1199: 20.0%\n",
      "1200: 15.0%\n",
      "1201: 0.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1202: 10.0%\n",
      "1203: 10.0%\n",
      "1204: 15.0%\n",
      "1205: 20.0%\n",
      "1206: 25.0%\n",
      "1207: 15.0%\n",
      "1208: 25.0%\n",
      "1209: 15.0%\n",
      "1210: 20.0%\n",
      "1211: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1212: 10.0%\n",
      "1213: 15.0%\n",
      "1214: 15.0%\n",
      "1215: 25.0%\n",
      "1216: 25.0%\n",
      "1217: 15.0%\n",
      "1218: 10.0%\n",
      "1219: 0.0%\n",
      "1220: 10.0%\n",
      "1221: 10.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1222: 30.0%\n",
      "1223: 20.0%\n",
      "1224: 10.0%\n",
      "1225: 15.0%\n",
      "1226: 15.0%\n",
      "1227: 5.0%\n",
      "1228: 10.0%\n",
      "1229: 10.0%\n",
      "1230: 5.0%\n",
      "1231: 20.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1232: 5.0%\n",
      "1233: 20.0%\n",
      "1234: 10.0%\n",
      "1235: 10.0%\n",
      "1236: 15.0%\n",
      "1237: 25.0%\n",
      "1238: 25.0%\n",
      "1239: 0.0%\n",
      "1240: 20.0%\n",
      "1241: 30.0%\n",
      "saving... att_checkpoints/simple-rnn-attention\n",
      "1242: 10.0%\n",
      "1243: 15.0%\n",
      "1244: 10.0%\n",
      "1245: 0.0%\n",
      "1246: 5.0%\n",
      "1247: 10.0%\n",
      "1248: 25.0%\n",
      "1249: 5.0%\n",
      "1250: 10.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kurbanov/Soft/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:143: DeprecationWarning: generator 'preprocess_batched' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "model.train(batches, save_prefix='simple-rnn-attention')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
