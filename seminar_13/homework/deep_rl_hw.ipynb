{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tflearn\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: fill empty spaces in the following agent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepQAgent:\n",
    "    def __init__(self, state_size, action_size, render=True):\n",
    "        # Tip: if you are training this on AWS the best way is to turn off rendering\n",
    "        # and load it later with the serialized model\n",
    "        self.render = render\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / 50000\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # replay memory\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        # Use tflearn to get simple NN for deep q-learning\n",
    "        # Spoler alert: a couple of fully connected hidden layers should be enough\n",
    "        # Output layer should have the same dimensionality as the action space\n",
    "        # TODO\n",
    "        \n",
    "#         in_ = input_data(shape=[self.batch_size, None], name='input')\n",
    "#         fc1 = fully_connected(in_, 10, activation='relu')\n",
    "#         fc2 = fully_connected(fc1, 10, activation='relu')\n",
    "#         out = fully_connected(fc2, 10, activation='softmax')\n",
    "#         reg = regression(out, optimizer='adam', learning_rate=0.01, loss='mse')\n",
    "#         return tflearn.DNN(reg)\n",
    "        \n",
    "        model = Sequential([\n",
    "            Dense(10, input_dim = self.state_size),\n",
    "            Activation('relu'),\n",
    "            Dense(10),\n",
    "            Activation('relu'),\n",
    "            Dense(self.action_size),\n",
    "            Activation('softmax'),\n",
    "        ])\n",
    "        \n",
    "        model.compile(sgd(lr=self.learning_rate), \"mse\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update your target model to the model you are currently learning at regular time intervals\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"The choice of action uses the epsilon-greedy policy for the current network.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save <s, a, r, s'> to replay_memory\"\"\"\n",
    "        if action == 2:\n",
    "            action = 1\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "            # print(len(self.memory))\n",
    "\n",
    "    def train_replay(self):\n",
    "        \"\"\"Random sampling of batch_size samples from replay memory\"\"\"\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.action_size))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = mini_batch[i]\n",
    "            target = self.model.predict(state)[0]\n",
    "\n",
    "            # As in queuing, it gets the maximum Q Value at s'. However, it is imported from the target model.\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target[action] = reward + self.discount_factor * \\\n",
    "                                          np.amax(self.target_model.predict(next_state)[0])\n",
    "            update_input[i] = state\n",
    "            update_target[i] = target\n",
    "\n",
    "        # You can create a minibatch of the correct target answer and the current value of your own,\n",
    "        self.model.fit(update_input, update_target, batch_size=batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.model.load_model(name)\n",
    "\n",
    "    def save_model(self, name):\n",
    "        self.model.save(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0] # should be equal 2\n",
    "ACTION_SIZE = 2\n",
    "agent = DeepQAgent(state_size, ACTION_SIZE, render=False)\n",
    "# agent.load_model(\"./save_model/<your_saved_model_name>\")\n",
    "scores, episodes = [], []\n",
    "N_EPISODES = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5745403  0.       ]]\n",
      "episode: 0   score: -200.0   memory length: 200   epsilon: 0.9960200000000077\n",
      "[[-0.5055888  0.       ]]\n",
      "episode: 1   score: -200.0   memory length: 400   epsilon: 0.9920400000000154\n",
      "[[-0.49055114  0.        ]]\n",
      "episode: 2   score: -200.0   memory length: 600   epsilon: 0.988060000000023\n",
      "[[-0.49603935  0.        ]]\n",
      "episode: 3   score: -200.0   memory length: 800   epsilon: 0.9840800000000307\n",
      "[[-0.57242767  0.        ]]\n",
      "episode: 4   score: -200.0   memory length: 1000   epsilon: 0.9801000000000384\n",
      "[[-0.57317671  0.        ]]\n",
      "episode: 5   score: -200.0   memory length: 1200   epsilon: 0.9761200000000461\n",
      "[[-0.40493141  0.        ]]\n",
      "episode: 6   score: -200.0   memory length: 1400   epsilon: 0.9721400000000537\n",
      "[[-0.52966852  0.        ]]\n",
      "episode: 7   score: -200.0   memory length: 1600   epsilon: 0.9681600000000614\n",
      "[[-0.45261726  0.        ]]\n",
      "episode: 8   score: -200.0   memory length: 1800   epsilon: 0.9641800000000691\n",
      "[[-0.56686632  0.        ]]\n",
      "episode: 9   score: -200.0   memory length: 2000   epsilon: 0.9602000000000768\n",
      "[[-0.46509949  0.        ]]\n",
      "episode: 10   score: -200.0   memory length: 2200   epsilon: 0.9562200000000844\n",
      "[[-0.4518051  0.       ]]\n",
      "episode: 11   score: -200.0   memory length: 2400   epsilon: 0.9522400000000921\n",
      "[[-0.49976047  0.        ]]\n",
      "episode: 12   score: -200.0   memory length: 2600   epsilon: 0.9482600000000998\n",
      "[[-0.59718973  0.        ]]\n",
      "episode: 13   score: -200.0   memory length: 2800   epsilon: 0.9442800000001075\n",
      "[[-0.48868013  0.        ]]\n",
      "episode: 14   score: -200.0   memory length: 3000   epsilon: 0.9403000000001152\n",
      "[[-0.5552771  0.       ]]\n",
      "episode: 15   score: -200.0   memory length: 3200   epsilon: 0.9363200000001228\n",
      "[[-0.54750807  0.        ]]\n",
      "episode: 16   score: -200.0   memory length: 3400   epsilon: 0.9323400000001305\n",
      "[[-0.43746103  0.        ]]\n",
      "episode: 17   score: -200.0   memory length: 3600   epsilon: 0.9283600000001382\n",
      "[[-0.52103016  0.        ]]\n",
      "episode: 18   score: -200.0   memory length: 3800   epsilon: 0.9243800000001459\n",
      "[[-0.43858235  0.        ]]\n",
      "episode: 19   score: -200.0   memory length: 4000   epsilon: 0.9204000000001535\n",
      "[[-0.56574782  0.        ]]\n",
      "episode: 20   score: -200.0   memory length: 4200   epsilon: 0.9164200000001612\n",
      "[[-0.56406541  0.        ]]\n",
      "episode: 21   score: -200.0   memory length: 4400   epsilon: 0.9124400000001689\n",
      "[[-0.45253989  0.        ]]\n",
      "episode: 22   score: -200.0   memory length: 4600   epsilon: 0.9084600000001766\n",
      "[[-0.42283133  0.        ]]\n",
      "episode: 23   score: -200.0   memory length: 4800   epsilon: 0.9044800000001842\n",
      "[[-0.52754944  0.        ]]\n",
      "episode: 24   score: -200.0   memory length: 5000   epsilon: 0.9005000000001919\n",
      "[[-0.56315143  0.        ]]\n",
      "episode: 25   score: -200.0   memory length: 5200   epsilon: 0.8965200000001996\n",
      "[[-0.43496995  0.        ]]\n",
      "episode: 26   score: -200.0   memory length: 5400   epsilon: 0.8925400000002073\n",
      "[[-0.46359131  0.        ]]\n",
      "episode: 27   score: -200.0   memory length: 5600   epsilon: 0.888560000000215\n",
      "[[-0.59450097  0.        ]]\n",
      "episode: 28   score: -200.0   memory length: 5800   epsilon: 0.8845800000002226\n",
      "[[-0.54795788  0.        ]]\n",
      "episode: 29   score: -200.0   memory length: 6000   epsilon: 0.8806000000002303\n",
      "[[-0.40310295  0.        ]]\n",
      "episode: 30   score: -200.0   memory length: 6200   epsilon: 0.876620000000238\n",
      "[[-0.48817961  0.        ]]\n",
      "episode: 31   score: -200.0   memory length: 6400   epsilon: 0.8726400000002457\n",
      "[[-0.44641619  0.        ]]\n",
      "episode: 32   score: -200.0   memory length: 6600   epsilon: 0.8686600000002533\n",
      "[[-0.57491174  0.        ]]\n",
      "episode: 33   score: -200.0   memory length: 6800   epsilon: 0.864680000000261\n",
      "[[-0.44275127  0.        ]]\n",
      "episode: 34   score: -200.0   memory length: 7000   epsilon: 0.8607000000002687\n",
      "[[-0.53501996  0.        ]]\n",
      "episode: 35   score: -200.0   memory length: 7200   epsilon: 0.8567200000002764\n",
      "[[-0.49426791  0.        ]]\n",
      "episode: 36   score: -200.0   memory length: 7400   epsilon: 0.852740000000284\n",
      "[[-0.4923279  0.       ]]\n",
      "episode: 37   score: -200.0   memory length: 7600   epsilon: 0.8487600000002917\n",
      "[[-0.59385989  0.        ]]\n",
      "episode: 38   score: -200.0   memory length: 7800   epsilon: 0.8447800000002994\n",
      "[[-0.56051065  0.        ]]\n",
      "episode: 39   score: -200.0   memory length: 8000   epsilon: 0.8408000000003071\n",
      "[[-0.48210313  0.        ]]\n",
      "episode: 40   score: -200.0   memory length: 8200   epsilon: 0.8368200000003148\n",
      "[[-0.41753284  0.        ]]\n",
      "episode: 41   score: -200.0   memory length: 8400   epsilon: 0.8328400000003224\n",
      "[[-0.53331325  0.        ]]\n",
      "episode: 42   score: -200.0   memory length: 8600   epsilon: 0.8288600000003301\n",
      "[[-0.48630586  0.        ]]\n",
      "episode: 43   score: -200.0   memory length: 8800   epsilon: 0.8248800000003378\n",
      "[[-0.44585658  0.        ]]\n",
      "episode: 44   score: -200.0   memory length: 9000   epsilon: 0.8209000000003455\n",
      "[[-0.5595832  0.       ]]\n",
      "episode: 45   score: -200.0   memory length: 9200   epsilon: 0.8169200000003531\n",
      "[[-0.59864927  0.        ]]\n",
      "episode: 46   score: -200.0   memory length: 9400   epsilon: 0.8129400000003608\n",
      "[[-0.58782779  0.        ]]\n",
      "episode: 47   score: -200.0   memory length: 9600   epsilon: 0.8089600000003685\n",
      "[[-0.56657069  0.        ]]\n",
      "episode: 48   score: -200.0   memory length: 9800   epsilon: 0.8049800000003762\n",
      "[[-0.41651609  0.        ]]\n",
      "episode: 49   score: -200.0   memory length: 10000   epsilon: 0.8010000000003838\n",
      "[[-0.44103191  0.        ]]\n",
      "episode: 50   score: -200.0   memory length: 10000   epsilon: 0.7970200000003915\n",
      "[[-0.54776884  0.        ]]\n",
      "episode: 51   score: -200.0   memory length: 10000   epsilon: 0.7930400000003992\n",
      "[[-0.42857972  0.        ]]\n",
      "episode: 52   score: -200.0   memory length: 10000   epsilon: 0.7890600000004069\n",
      "[[-0.59436034  0.        ]]\n",
      "episode: 53   score: -200.0   memory length: 10000   epsilon: 0.7850800000004146\n",
      "[[-0.4154557  0.       ]]\n",
      "episode: 54   score: -200.0   memory length: 10000   epsilon: 0.7811000000004222\n",
      "[[-0.53416863  0.        ]]\n",
      "episode: 55   score: -200.0   memory length: 10000   epsilon: 0.7771200000004299\n",
      "[[-0.5730294  0.       ]]\n",
      "episode: 56   score: -200.0   memory length: 10000   epsilon: 0.7731400000004376\n",
      "[[-0.52703418  0.        ]]\n",
      "episode: 57   score: -200.0   memory length: 10000   epsilon: 0.7691600000004453\n",
      "[[-0.40806298  0.        ]]\n",
      "episode: 58   score: -200.0   memory length: 10000   epsilon: 0.7651800000004529\n",
      "[[-0.53122502  0.        ]]\n",
      "episode: 59   score: -200.0   memory length: 10000   epsilon: 0.7612000000004606\n",
      "[[-0.46303526  0.        ]]\n",
      "episode: 60   score: -200.0   memory length: 10000   epsilon: 0.7572200000004683\n",
      "[[-0.540195  0.      ]]\n",
      "episode: 61   score: -200.0   memory length: 10000   epsilon: 0.753240000000476\n",
      "[[-0.57721308  0.        ]]\n",
      "episode: 62   score: -200.0   memory length: 10000   epsilon: 0.7492600000004837\n",
      "[[-0.41512025  0.        ]]\n",
      "episode: 63   score: -200.0   memory length: 10000   epsilon: 0.7452800000004913\n",
      "[[-0.54565369  0.        ]]\n",
      "episode: 64   score: -200.0   memory length: 10000   epsilon: 0.741300000000499\n",
      "[[-0.47359838  0.        ]]\n",
      "episode: 65   score: -200.0   memory length: 10000   epsilon: 0.7373200000005067\n",
      "[[-0.43825482  0.        ]]\n",
      "episode: 66   score: -200.0   memory length: 10000   epsilon: 0.7333400000005144\n",
      "[[-0.49729574  0.        ]]\n",
      "episode: 67   score: -200.0   memory length: 10000   epsilon: 0.729360000000522\n",
      "[[-0.55998257  0.        ]]\n",
      "episode: 68   score: -200.0   memory length: 10000   epsilon: 0.7253800000005297\n",
      "[[-0.48759163  0.        ]]\n",
      "episode: 69   score: -200.0   memory length: 10000   epsilon: 0.7214000000005374\n",
      "[[-0.43856081  0.        ]]\n",
      "episode: 70   score: -200.0   memory length: 10000   epsilon: 0.7174200000005451\n",
      "[[-0.44726338  0.        ]]\n",
      "episode: 71   score: -200.0   memory length: 10000   epsilon: 0.7134400000005527\n",
      "[[-0.51252688  0.        ]]\n",
      "episode: 72   score: -200.0   memory length: 10000   epsilon: 0.7094600000005604\n",
      "[[-0.42221676  0.        ]]\n",
      "episode: 73   score: -200.0   memory length: 10000   epsilon: 0.7054800000005681\n",
      "[[-0.44863421  0.        ]]\n",
      "episode: 74   score: -200.0   memory length: 10000   epsilon: 0.7015000000005758\n",
      "[[-0.47035281  0.        ]]\n",
      "episode: 75   score: -200.0   memory length: 10000   epsilon: 0.6975200000005835\n",
      "[[-0.50291177  0.        ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 76   score: -200.0   memory length: 10000   epsilon: 0.6935400000005911\n",
      "[[-0.44175735  0.        ]]\n",
      "episode: 77   score: -200.0   memory length: 10000   epsilon: 0.6895600000005988\n",
      "[[-0.49696232  0.        ]]\n",
      "episode: 78   score: -200.0   memory length: 10000   epsilon: 0.6855800000006065\n",
      "[[-0.58544388  0.        ]]\n",
      "episode: 79   score: -200.0   memory length: 10000   epsilon: 0.6816000000006142\n",
      "[[-0.50255163  0.        ]]\n",
      "episode: 80   score: -200.0   memory length: 10000   epsilon: 0.6776200000006218\n",
      "[[-0.43204187  0.        ]]\n",
      "episode: 81   score: -200.0   memory length: 10000   epsilon: 0.6736400000006295\n",
      "[[-0.47025562  0.        ]]\n",
      "episode: 82   score: -200.0   memory length: 10000   epsilon: 0.6696600000006372\n",
      "[[-0.41836766  0.        ]]\n",
      "episode: 83   score: -200.0   memory length: 10000   epsilon: 0.6656800000006449\n",
      "[[-0.55859248  0.        ]]\n",
      "episode: 84   score: -200.0   memory length: 10000   epsilon: 0.6617000000006525\n",
      "[[-0.50790375  0.        ]]\n",
      "episode: 85   score: -200.0   memory length: 10000   epsilon: 0.6577200000006602\n",
      "[[-0.54633089  0.        ]]\n",
      "episode: 86   score: -200.0   memory length: 10000   epsilon: 0.6537400000006679\n",
      "[[-0.57527173  0.        ]]\n",
      "episode: 87   score: -200.0   memory length: 10000   epsilon: 0.6497600000006756\n",
      "[[-0.52930873  0.        ]]\n",
      "episode: 88   score: -200.0   memory length: 10000   epsilon: 0.6457800000006833\n",
      "[[-0.57195934  0.        ]]\n",
      "episode: 89   score: -200.0   memory length: 10000   epsilon: 0.6418000000006909\n",
      "[[-0.50034467  0.        ]]\n",
      "episode: 90   score: -200.0   memory length: 10000   epsilon: 0.6378200000006986\n",
      "[[-0.59450206  0.        ]]\n",
      "episode: 91   score: -200.0   memory length: 10000   epsilon: 0.6338400000007063\n",
      "[[-0.41627153  0.        ]]\n",
      "episode: 92   score: -200.0   memory length: 10000   epsilon: 0.629860000000714\n",
      "[[-0.48067645  0.        ]]\n",
      "episode: 93   score: -200.0   memory length: 10000   epsilon: 0.6258800000007216\n",
      "[[-0.47658485  0.        ]]\n",
      "episode: 94   score: -200.0   memory length: 10000   epsilon: 0.6219000000007293\n",
      "[[-0.44362336  0.        ]]\n",
      "episode: 95   score: -200.0   memory length: 10000   epsilon: 0.617920000000737\n",
      "[[-0.53169719  0.        ]]\n",
      "episode: 96   score: -200.0   memory length: 10000   epsilon: 0.6139400000007447\n",
      "[[-0.46705348  0.        ]]\n",
      "episode: 97   score: -200.0   memory length: 10000   epsilon: 0.6099600000007523\n",
      "[[-0.55439556  0.        ]]\n",
      "episode: 98   score: -200.0   memory length: 10000   epsilon: 0.60598000000076\n",
      "[[-0.40099208  0.        ]]\n",
      "episode: 99   score: -200.0   memory length: 10000   epsilon: 0.6020000000007677\n",
      "[[-0.42788015  0.        ]]\n",
      "episode: 100   score: -200.0   memory length: 10000   epsilon: 0.5980200000007754\n",
      "[[-0.41604578  0.        ]]\n",
      "episode: 101   score: -200.0   memory length: 10000   epsilon: 0.594040000000783\n",
      "[[-0.45013497  0.        ]]\n",
      "episode: 102   score: -200.0   memory length: 10000   epsilon: 0.5900600000007907\n",
      "[[-0.41835737  0.        ]]\n",
      "episode: 103   score: -200.0   memory length: 10000   epsilon: 0.5860800000007984\n",
      "[[-0.48826515  0.        ]]\n",
      "episode: 104   score: -200.0   memory length: 10000   epsilon: 0.5821000000008061\n",
      "[[-0.43904395  0.        ]]\n",
      "episode: 105   score: -200.0   memory length: 10000   epsilon: 0.5781200000008138\n",
      "[[-0.53306492  0.        ]]\n",
      "episode: 106   score: -200.0   memory length: 10000   epsilon: 0.5741400000008214\n",
      "[[-0.55903758  0.        ]]\n",
      "episode: 107   score: -200.0   memory length: 10000   epsilon: 0.5701600000008291\n",
      "[[-0.4053553  0.       ]]\n",
      "episode: 108   score: -200.0   memory length: 10000   epsilon: 0.5661800000008368\n",
      "[[-0.53121553  0.        ]]\n",
      "episode: 109   score: -200.0   memory length: 10000   epsilon: 0.5622000000008445\n",
      "[[-0.44880264  0.        ]]\n",
      "episode: 110   score: -200.0   memory length: 10000   epsilon: 0.5582200000008521\n",
      "[[-0.4058884  0.       ]]\n",
      "episode: 111   score: -200.0   memory length: 10000   epsilon: 0.5542400000008598\n",
      "[[-0.46952031  0.        ]]\n",
      "episode: 112   score: -200.0   memory length: 10000   epsilon: 0.5502600000008675\n",
      "[[-0.56224322  0.        ]]\n",
      "episode: 113   score: -200.0   memory length: 10000   epsilon: 0.5462800000008752\n",
      "[[-0.49560655  0.        ]]\n",
      "episode: 114   score: -200.0   memory length: 10000   epsilon: 0.5423000000008829\n",
      "[[-0.45730998  0.        ]]\n",
      "episode: 115   score: -200.0   memory length: 10000   epsilon: 0.5383200000008905\n",
      "[[-0.50171077  0.        ]]\n",
      "episode: 116   score: -200.0   memory length: 10000   epsilon: 0.5343400000008982\n",
      "[[-0.5527579  0.       ]]\n",
      "episode: 117   score: -200.0   memory length: 10000   epsilon: 0.5303600000009059\n",
      "[[-0.49787566  0.        ]]\n",
      "episode: 118   score: -200.0   memory length: 10000   epsilon: 0.5263800000009136\n",
      "[[-0.57213283  0.        ]]\n",
      "episode: 119   score: -200.0   memory length: 10000   epsilon: 0.5224000000009212\n",
      "[[-0.51576135  0.        ]]\n",
      "episode: 120   score: -200.0   memory length: 10000   epsilon: 0.5184200000009289\n",
      "[[-0.46868203  0.        ]]\n",
      "episode: 121   score: -200.0   memory length: 10000   epsilon: 0.5144400000009366\n",
      "[[-0.54261278  0.        ]]\n",
      "episode: 122   score: -200.0   memory length: 10000   epsilon: 0.5104600000009443\n",
      "[[-0.40953183  0.        ]]\n",
      "episode: 123   score: -200.0   memory length: 10000   epsilon: 0.506480000000952\n",
      "[[-0.54586411  0.        ]]\n",
      "episode: 124   score: -200.0   memory length: 10000   epsilon: 0.5025000000009596\n",
      "[[-0.56216042  0.        ]]\n",
      "episode: 125   score: -200.0   memory length: 10000   epsilon: 0.49852000000096314\n",
      "[[-0.57416028  0.        ]]\n",
      "episode: 126   score: -200.0   memory length: 10000   epsilon: 0.4945400000009597\n",
      "[[-0.53673348  0.        ]]\n",
      "episode: 127   score: -200.0   memory length: 10000   epsilon: 0.4905600000009563\n",
      "[[-0.51546914  0.        ]]\n",
      "episode: 128   score: -200.0   memory length: 10000   epsilon: 0.48658000000095286\n",
      "[[-0.53743013  0.        ]]\n",
      "episode: 129   score: -200.0   memory length: 10000   epsilon: 0.48260000000094944\n",
      "[[-0.50779753  0.        ]]\n",
      "episode: 130   score: -200.0   memory length: 10000   epsilon: 0.478620000000946\n",
      "[[-0.43454358  0.        ]]\n",
      "episode: 131   score: -200.0   memory length: 10000   epsilon: 0.4746400000009426\n",
      "[[-0.41724046  0.        ]]\n",
      "episode: 132   score: -200.0   memory length: 10000   epsilon: 0.47066000000093916\n",
      "[[-0.46057497  0.        ]]\n",
      "episode: 133   score: -200.0   memory length: 10000   epsilon: 0.46668000000093574\n",
      "[[-0.40412684  0.        ]]\n",
      "episode: 134   score: -200.0   memory length: 10000   epsilon: 0.4627000000009323\n",
      "[[-0.4470902  0.       ]]\n",
      "episode: 135   score: -200.0   memory length: 10000   epsilon: 0.4587200000009289\n",
      "[[-0.54283042  0.        ]]\n",
      "episode: 136   score: -200.0   memory length: 10000   epsilon: 0.45474000000092546\n",
      "[[-0.41785517  0.        ]]\n",
      "episode: 137   score: -200.0   memory length: 10000   epsilon: 0.45076000000092203\n",
      "[[-0.42763757  0.        ]]\n",
      "episode: 138   score: -200.0   memory length: 10000   epsilon: 0.4467800000009186\n",
      "[[-0.53987017  0.        ]]\n",
      "episode: 139   score: -200.0   memory length: 10000   epsilon: 0.4428000000009152\n",
      "[[-0.5118865  0.       ]]\n",
      "episode: 140   score: -200.0   memory length: 10000   epsilon: 0.43882000000091176\n",
      "[[-0.44191452  0.        ]]\n",
      "episode: 141   score: -200.0   memory length: 10000   epsilon: 0.43484000000090833\n",
      "[[-0.53238495  0.        ]]\n",
      "episode: 142   score: -200.0   memory length: 10000   epsilon: 0.4308600000009049\n",
      "[[-0.55118305  0.        ]]\n",
      "episode: 143   score: -200.0   memory length: 10000   epsilon: 0.4268800000009015\n",
      "[[-0.55660232  0.        ]]\n",
      "episode: 144   score: -200.0   memory length: 10000   epsilon: 0.42290000000089806\n",
      "[[-0.5439809  0.       ]]\n",
      "episode: 145   score: -200.0   memory length: 10000   epsilon: 0.41892000000089463\n",
      "[[-0.46794196  0.        ]]\n",
      "episode: 146   score: -200.0   memory length: 10000   epsilon: 0.4149400000008912\n",
      "[[-0.48715712  0.        ]]\n",
      "episode: 147   score: -200.0   memory length: 10000   epsilon: 0.4109600000008878\n",
      "[[-0.42473549  0.        ]]\n",
      "episode: 148   score: -200.0   memory length: 10000   epsilon: 0.40698000000088436\n",
      "[[-0.59607166  0.        ]]\n",
      "episode: 149   score: -200.0   memory length: 10000   epsilon: 0.40300000000088093\n",
      "[[-0.5081506  0.       ]]\n",
      "episode: 150   score: -200.0   memory length: 10000   epsilon: 0.3990200000008775\n",
      "[[-0.44376966  0.        ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 151   score: -200.0   memory length: 10000   epsilon: 0.3950400000008741\n",
      "[[-0.46158544  0.        ]]\n",
      "episode: 152   score: -200.0   memory length: 10000   epsilon: 0.39106000000087066\n",
      "[[-0.53072626  0.        ]]\n",
      "episode: 153   score: -200.0   memory length: 10000   epsilon: 0.38708000000086723\n",
      "[[-0.56244611  0.        ]]\n",
      "episode: 154   score: -200.0   memory length: 10000   epsilon: 0.3831000000008638\n",
      "[[-0.5118817  0.       ]]\n",
      "episode: 155   score: -200.0   memory length: 10000   epsilon: 0.3791200000008604\n",
      "[[-0.56910172  0.        ]]\n",
      "episode: 156   score: -200.0   memory length: 10000   epsilon: 0.37514000000085695\n",
      "[[-0.57247662  0.        ]]\n",
      "episode: 157   score: -200.0   memory length: 10000   epsilon: 0.37116000000085353\n",
      "[[-0.47689854  0.        ]]\n",
      "episode: 158   score: -200.0   memory length: 10000   epsilon: 0.3671800000008501\n",
      "[[-0.59426685  0.        ]]\n",
      "episode: 159   score: -200.0   memory length: 10000   epsilon: 0.3632000000008467\n",
      "[[-0.43865531  0.        ]]\n",
      "episode: 160   score: -200.0   memory length: 10000   epsilon: 0.35922000000084325\n",
      "[[-0.58622764  0.        ]]\n",
      "episode: 161   score: -200.0   memory length: 10000   epsilon: 0.35524000000083983\n",
      "[[-0.59683955  0.        ]]\n",
      "episode: 162   score: -200.0   memory length: 10000   epsilon: 0.3512600000008364\n",
      "[[-0.5732143  0.       ]]\n",
      "episode: 163   score: -200.0   memory length: 10000   epsilon: 0.347280000000833\n",
      "[[-0.45490542  0.        ]]\n",
      "episode: 164   score: -200.0   memory length: 10000   epsilon: 0.34330000000082955\n",
      "[[-0.58436994  0.        ]]\n",
      "episode: 165   score: -200.0   memory length: 10000   epsilon: 0.3393200000008261\n",
      "[[-0.40297964  0.        ]]\n",
      "episode: 166   score: -200.0   memory length: 10000   epsilon: 0.3353400000008227\n",
      "[[-0.44919143  0.        ]]\n",
      "episode: 167   score: -200.0   memory length: 10000   epsilon: 0.3313600000008193\n",
      "[[-0.55445954  0.        ]]\n",
      "episode: 168   score: -200.0   memory length: 10000   epsilon: 0.32738000000081585\n",
      "[[-0.46545473  0.        ]]\n",
      "episode: 169   score: -200.0   memory length: 10000   epsilon: 0.3234000000008124\n",
      "[[-0.46338858  0.        ]]\n",
      "episode: 170   score: -200.0   memory length: 10000   epsilon: 0.319420000000809\n",
      "[[-0.47782121  0.        ]]\n",
      "episode: 171   score: -200.0   memory length: 10000   epsilon: 0.3154400000008056\n",
      "[[-0.41913385  0.        ]]\n",
      "episode: 172   score: -200.0   memory length: 10000   epsilon: 0.31146000000080215\n",
      "[[-0.52590667  0.        ]]\n",
      "episode: 173   score: -200.0   memory length: 10000   epsilon: 0.3074800000007987\n",
      "[[-0.59238846  0.        ]]\n",
      "episode: 174   score: -200.0   memory length: 10000   epsilon: 0.3035000000007953\n",
      "[[-0.41018429  0.        ]]\n",
      "episode: 175   score: -200.0   memory length: 10000   epsilon: 0.2995200000007919\n",
      "[[-0.45755117  0.        ]]\n",
      "episode: 176   score: -200.0   memory length: 10000   epsilon: 0.29554000000078845\n",
      "[[-0.52514417  0.        ]]\n",
      "episode: 177   score: -200.0   memory length: 10000   epsilon: 0.291560000000785\n",
      "[[-0.54322511  0.        ]]\n",
      "episode: 178   score: -200.0   memory length: 10000   epsilon: 0.2875800000007816\n",
      "[[-0.43393055  0.        ]]\n",
      "episode: 179   score: -200.0   memory length: 10000   epsilon: 0.2836000000007782\n",
      "[[-0.43585804  0.        ]]\n",
      "episode: 180   score: -200.0   memory length: 10000   epsilon: 0.27962000000077475\n",
      "[[-0.48028902  0.        ]]\n",
      "episode: 181   score: -200.0   memory length: 10000   epsilon: 0.2756400000007713\n",
      "[[-0.4841121  0.       ]]\n",
      "episode: 182   score: -200.0   memory length: 10000   epsilon: 0.2716600000007679\n",
      "[[-0.57091273  0.        ]]\n",
      "episode: 183   score: -200.0   memory length: 10000   epsilon: 0.2676800000007645\n",
      "[[-0.57386083  0.        ]]\n",
      "episode: 184   score: -200.0   memory length: 10000   epsilon: 0.26370000000076105\n",
      "[[-0.45193796  0.        ]]\n",
      "episode: 185   score: -200.0   memory length: 10000   epsilon: 0.2597200000007576\n",
      "[[-0.55992739  0.        ]]\n",
      "episode: 186   score: -200.0   memory length: 10000   epsilon: 0.2557400000007542\n",
      "[[-0.41740626  0.        ]]\n",
      "episode: 187   score: -200.0   memory length: 10000   epsilon: 0.25176000000075077\n",
      "[[-0.51208044  0.        ]]\n",
      "episode: 188   score: -200.0   memory length: 10000   epsilon: 0.24778000000075046\n",
      "[[-0.59085667  0.        ]]\n",
      "episode: 189   score: -200.0   memory length: 10000   epsilon: 0.24380000000075258\n",
      "[[-0.43321622  0.        ]]\n",
      "episode: 190   score: -200.0   memory length: 10000   epsilon: 0.2398200000007547\n",
      "[[-0.50386295  0.        ]]\n",
      "episode: 191   score: -200.0   memory length: 10000   epsilon: 0.23584000000075683\n",
      "[[-0.43229958  0.        ]]\n",
      "episode: 192   score: -200.0   memory length: 10000   epsilon: 0.23186000000075896\n",
      "[[-0.54925491  0.        ]]\n",
      "episode: 193   score: -200.0   memory length: 10000   epsilon: 0.22788000000076108\n",
      "[[-0.52613335  0.        ]]\n",
      "episode: 194   score: -200.0   memory length: 10000   epsilon: 0.2239000000007632\n",
      "[[-0.4079983  0.       ]]\n",
      "episode: 195   score: -200.0   memory length: 10000   epsilon: 0.21992000000076534\n",
      "[[-0.44111404  0.        ]]\n",
      "episode: 196   score: -200.0   memory length: 10000   epsilon: 0.21594000000076746\n",
      "[[-0.4975511  0.       ]]\n",
      "episode: 197   score: -200.0   memory length: 10000   epsilon: 0.2119600000007696\n",
      "[[-0.49485702  0.        ]]\n",
      "episode: 198   score: -200.0   memory length: 10000   epsilon: 0.20798000000077171\n",
      "[[-0.55939484  0.        ]]\n",
      "episode: 199   score: -200.0   memory length: 10000   epsilon: 0.20400000000077384\n",
      "[[-0.54072782  0.        ]]\n",
      "episode: 200   score: -200.0   memory length: 10000   epsilon: 0.20002000000077597\n",
      "[[-0.4838691  0.       ]]\n",
      "episode: 201   score: -200.0   memory length: 10000   epsilon: 0.1960400000007781\n",
      "[[-0.43493393  0.        ]]\n",
      "episode: 202   score: -200.0   memory length: 10000   epsilon: 0.19206000000078022\n",
      "[[-0.53959648  0.        ]]\n",
      "episode: 203   score: -200.0   memory length: 10000   epsilon: 0.18808000000078234\n",
      "[[-0.44599026  0.        ]]\n",
      "episode: 204   score: -200.0   memory length: 10000   epsilon: 0.18410000000078447\n",
      "[[-0.44322448  0.        ]]\n",
      "episode: 205   score: -200.0   memory length: 10000   epsilon: 0.1801200000007866\n",
      "[[-0.42743537  0.        ]]\n",
      "episode: 206   score: -200.0   memory length: 10000   epsilon: 0.17614000000078872\n",
      "[[-0.57762939  0.        ]]\n",
      "episode: 207   score: -200.0   memory length: 10000   epsilon: 0.17216000000079085\n",
      "[[-0.45067968  0.        ]]\n",
      "episode: 208   score: -200.0   memory length: 10000   epsilon: 0.16818000000079297\n",
      "[[-0.43476301  0.        ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b1022e8f1bba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Continue to learn every time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-274ada9bce23>\u001b[0m in \u001b[0;36mtrain_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m                                           \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mupdate_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mupdate_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1592\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1594\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1216\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(N_EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    print(state)\n",
    "\n",
    "    # Action 0 (left), 1 (do nothing), 3 (declare fake_action to avoid doing nothing\n",
    "    fake_action = 0\n",
    "\n",
    "    # Counter for the same action 4 times\n",
    "    action_count = 0\n",
    "\n",
    "    while not done:\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        # Select an action in the current state and proceed to a step\n",
    "        action_count = action_count + 1\n",
    "\n",
    "        if action_count == 4:\n",
    "            action = agent.get_action(state)\n",
    "            action_count = 0\n",
    "\n",
    "            if action == 0:\n",
    "                fake_action = 0\n",
    "            elif action == 1:\n",
    "                fake_action = 2\n",
    "\n",
    "        # Take 1 step with the selected action\n",
    "        next_state, reward, done, info = env.step(fake_action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # Give a penalty of -100 for actions that end an episode\n",
    "        # reward = reward if not done else -100\n",
    "\n",
    "        # Save <s, a, r, s'> to replay memory\n",
    "        agent.replay_memory(state, fake_action, reward, next_state, done)\n",
    "        # Continue to learn every time step\n",
    "        agent.train_replay()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            # Copy the learning model for each episode to the target model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # For each episode, the time step where cartpole stood is plot\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\", len(agent.memory),\n",
    "                  \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "    # Save model for every 50 episodes\n",
    "    if e % 50 == 0:\n",
    "        agent.save_model(\"./save_model/<your_saved_model_name>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
