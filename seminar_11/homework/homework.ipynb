{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework:\n",
    "# Deep Convolutional Generative Adversarial Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of an implementation of DCGAN can be found in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the MNIST dataset. input_data is a library that downloads the dataset and uzips it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fashion-mnist/data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting fashion-mnist/data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting fashion-mnist/data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting fashion-mnist/data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('fashion-mnist/data/fashion', one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    \n",
    "    zP = slim.fully_connected(z,4*4*256,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,256])\n",
    "    \n",
    "    gen1 = slim.convolution2d_transpose(\\\n",
    "        zCon,num_outputs=64,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    \n",
    "    gen2 = slim.convolution2d_transpose(\\\n",
    "        gen1,num_outputs=32,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    gen3 = slim.convolution2d_transpose(\\\n",
    "        gen2,num_outputs=16,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    \n",
    "    g_out = slim.convolution2d_transpose(\\\n",
    "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 (13 points)\n",
    "Fill parameter for the discrimiator architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, reuse=False):\n",
    "    with slim.arg_scope([slim.fully_connected, slim.convolution2d], reuse=reuse, weights_initializer=initializer):\n",
    "        # Your code here\n",
    "        dis1 = slim.convolution2d(bottom, scope='d_conv1', num_outputs=16, kernel_size=[32,32], \n",
    "                                  padding=\"SAME\",normalizer_fn=slim.batch_norm, activation_fn=lrelu,\n",
    "                                  weights_initializer=initializer)\n",
    "\n",
    "        dis2 = slim.convolution2d(dis1, scope='d_conv2', num_outputs=32, kernel_size=[5,5],stride=[2,2], \n",
    "                                  padding=\"SAME\",normalizer_fn=slim.batch_norm, activation_fn=lrelu,\n",
    "                                  weights_initializer=initializer)\n",
    "\n",
    "        dis3 = slim.convolution2d(dis2,scope='d_conv3', num_outputs=64, kernel_size=[5,5],stride=[2,2], \n",
    "                                  padding=\"SAME\",normalizer_fn=slim.batch_norm, activation_fn=lrelu,\n",
    "                                  weights_initializer=initializer)\n",
    "\n",
    "        dis4 = slim.convolution2d(dis3, scope='d_conv4', num_outputs=128, kernel_size=[5, 5], stride=[2, 2],\n",
    "                                 padding=\"SAME\",normalizer_fn=slim.batch_norm, activation_fn=lrelu,\n",
    "                                 weights_initializer=initializer)\n",
    "        \n",
    "        d_out = slim.fully_connected(slim.flatten(dis4), scope='d_out', num_outputs=1, \n",
    "                                     activation_fn=tf.nn.sigmoid,weights_initializer=initializer)\n",
    "    \n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 100 #Size of z vector used for generator.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These two placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
    "\n",
    "Gz = generator(z_in) #Generates images from random z vectors\n",
    "Dx = discriminator(real_in) #Produces probabilities for real images\n",
    "Dg = discriminator(Gz,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss,tvars[0:9]) #Only update the weights for the generator network.\n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "I strongly advise you to skip this cell and go the the next one since training will take you enormous amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Gen Loss: 0.475512 Disc Loss: 1.04983\n",
      "10\n",
      "Gen Loss: 1.05356 Disc Loss: 1.99206\n",
      "Saved Model\n",
      "20\n",
      "Gen Loss: 1.147 Disc Loss: 2.16827\n",
      "Saved Model\n",
      "30\n",
      "Gen Loss: 0.898868 Disc Loss: 1.61016\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ac0ff8124f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Pad the images so the are 32x32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the generator, twice for good measure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128 #Size of image batch to apply at each iteration.\n",
    "iterations = 500000 #Total number of iterations to use.\n",
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs}) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs}) #Update the generator, twice for good measure.\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs})\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "            print(\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss))\n",
    "            z2 = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate another z batch\n",
    "            newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            save_images(np.reshape(newZ[0:36],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 10 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/model-20000.cptk\n"
     ]
    }
   ],
   "source": [
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "batch_size_sample = 36\n",
    "\n",
    "path = model_directory + '/model-20000.cptk'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    #Reload the model.\n",
    "    print( 'Loading Model...')\n",
    "    \n",
    "    saver.restore(sess, save_path=path)\n",
    "#     ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "#     saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    z2 = np.random.uniform(-1.0,1.0,size=[batch_size_sample,z_size]).astype(np.float32) #Generate a random z batch\n",
    "    newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    save_images(np.reshape(newZ[0:batch_size_sample],[36,32,32]),[6,6], sample_directory+'/fig.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (2 points)\n",
    "Run a couple of iterations and visualize examples generated by the generator (Could be found in ./fig folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMAAAADACAAAAAB3tzPbAAAjCUlEQVR4nOV9d3xUVfb4ue+9KWkk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\nqgzlaEHRunYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='figs/fig.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (5 points)\n",
    "Evaluate discrimator accuracy in the pre-trained model on any representative subsample of fashion-minst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/model-20000.cptk\n",
      "Accuracy on 10000 samples: 0.8685\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "samples_count = 10000\n",
    "\n",
    "path = model_directory + '/model-20000.cptk'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    #Reload the model.\n",
    "    print( 'Loading Model...')\n",
    "    \n",
    "    saver.restore(sess, save_path=path)\n",
    "    acc = 0\n",
    "\n",
    "    mnist_batch, _ = mnist.test.next_batch(samples_count)\n",
    "    mnist_batch = (np.reshape(mnist_batch,[samples_count,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "    mnist_batch = np.lib.pad(mnist_batch, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1))\n",
    "    res = sess.run(Dx, feed_dict={real_in: mnist_batch})\n",
    "    acc += (res >= 0.5).sum()\n",
    "\n",
    "    z2 = np.random.uniform(-1.0,1.0,size=[samples_count, z_size]).astype(np.float32) #Generate a random z batch\n",
    "    gen_batch = sess.run(Gz,feed_dict={z_in:z2})\n",
    "    res = sess.run(Dx, feed_dict={real_in: gen_batch})\n",
    "\n",
    "    acc += (res < 0.5).sum()\n",
    "    \n",
    "    acc /= (2*samples_count)\n",
    "    print('Accuracy on {} samples: {}'.format(samples_count, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
